<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>My "Reinforcement Learning" Notes - Domingo Esteban</title>
  <meta name="keywords" content="Domingo, Esteban, Cabala, Robotics, Peru, Spain, Madrid, Arequipa, Machine, Learning, Reinforcement">
  <meta name="description" content="Reinforcement Learning Notes - Domingo Esteban">
  <meta name="author" content="Domingo Esteban">

  <!-- Mobile viewport optimized -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
  <!--meta name="viewport" content="width=device-width, initial-scale=1"-->

  <!-- Bootstrap CSS -->
  <!-- <link rel="stylesheet" href="../framework/css/bootstrap.css"> -->
  <link href='http://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css' rel='stylesheet'/>
  <link href='http://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css' rel='stylesheet'/>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../includes/css/styles.css"/>
  <link rel="stylesheet" href="../../includes/css/tutorials-styles.css"/>

  <!-- Including Modernizr (Before any other Javascript) -->

  <!-- Favicon -->
  <link href="../../images/icons/robot-favicon.ico" rel="icon" type="image/x-icon" />

  <!-- MathJax  -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head>

<body>


<section class="tutorial-block-intro" style="background-color:#F9F7EB;">
  <div class="container">
    <h1 align="center">Reinforcement Learning</h1>

    <h2>The world</h2>

    <ul>
      <li><a href="#markovDecisionProcesses">Markov Decision Processes</a></li>
      <li><a href="#reinforcementLearning">Reinforcement Learning</a></li>
      <li><a href="#gameTheory">Game Theory</a></li>
    </ul>

    <div class="page-header">
      <h1 id="markovDecisionProcesses">Markov Decision Processes</h1>
    </div>

    <table class="table table-bordered text-center">
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&nbsp;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&nbsp;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </table> 

    <h2>Markov Decision Processes</h2>

    <p>The Markovian property in simple words means that only the present matter.</p>
    <p>The second property is that things are <i>stationary</i>. It means that rules don't change over time.</p>

    <ul>
      <li><b>States: $S$</b> . States are a set of tokens that represent every state that one could be in.</li>
      <li><b>Model: $T(s,a,s') \sim Pr(s'|s,a)$</b> . Also called the <i>Transition Model</i> or <i>Transition Function</i>, it is a function of 3 variables: a state, an action and another state. The function produces the probability that the agent will end up transitioning the state <var>s'</var> given the state <var>s</var> and the action <var>a</var>.</li>
      <li><b>Actions: $A(s)$, $A$</b> . The things that an agent can do in a particular state.</li>
      <li><b>Reward: $R(s)$, $R(s,a)$, $R(s,a,s')$</b> . It is a scalar value that you get for being in a state. There is also a notion of reward that you get for entering into a state and taking an action. And there is a reward that you get for being in a state, taking an action, and then ending in another state.</li>
      <li><b>Policy: $\pi (s) \rightarrow a $</b> . A policy is a function that takes in a state and returns an action. In other words, for any given state it tells you the action that you should take.</li>
    </ul>

    <p>The states, the model, the actions and the reward, along with the Markov properties, defines what is called the <b>Markov Decission Process</b> or <b>MDP</b> </p>

    <p>The solution to the Markov Decission Process is something called a Policy.</p>

    <p>Thre is an special policy, <b>$\pi^*$</b> or <b>Optimal Policy</b>, that is the policy that maximizes your long-term expected reward.</p>

    <p>If we want to learn a policy it is necessary a bunch of $\lt s,a\gt$ examples. With these, we can learn a function, the policy, that maps states to actions.</p>

    <p>But in the reinforcement world, int the MDP world, we see a sequence of states, actions and rewards $\lt s,a,r\gt$. In other words we have "if we are in the state <var>$s$</var>, and we take the action <var>$a$</var>, the reward that we see is <var>$r$</var> ", and from that we need to find the optimal action.</p>

    <p>A MDP, a policy dont tell you what sequence of actions (a plan) to take from a particular state, it tells you what action to take in a particular state. You will then end up in another state because of the transition model, the transition function. And then you are in that state you ask the policy what actions should I take now.</p>

    <h2>More About Rewards</h2>

    <p>There's a problem where you take some action that puts you in some place and then you take another action and that puts you in some other place, and maybe it puts you at a place where you get +1 or maybe it puts you at a place where you get -1. This reward is noy just an idea of getting a reward at every state, it's an idea of getting delayed reward. So you don't know how your immediate action is going to lead to things down the road.</p>

    <p>You get a sequence of $\lt s,a,r \gt $ triplets, and ultimately you have to figure out for the given state ( <var>$s$</var> ) you'are in, what was the action or actions ( <var>$a$</var> ) you took that helped to determinate the ultimate sequence of rewards that you saw. Perhaps the +1 or the -1. you got at the end. This is a difficult problem and as we are talking about a sequence of events over time its called <b>the credit assignment problem</b>.</p>

    <p>If we set the rewards to be equal to -0.04, $R(s) = -0.04 $ . So for the example, this is the reward that we are going to receive in every single state is -0.04, except for the two states +1 and -1 that ends the game (the <b>terminating</b> or <b>absorbing states</b>).
    </p>

    <p>If the world example now considers these sets of rewards:</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&uarr;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&uarr;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&uarr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
      </tr>
    </table> 

    <p>If we take a look at (3,1) cell, it has a left arrow because the policy tries to avoid the (3,2) cell that is to the right of the -1. Remember the world is stochastic, so there is a chance of moving to the right and following into the -1.</p>

    <p>So due the world has two assumptions: 1) delayed reward, and 2) minor changes matter, so if we have a slightly different reward, the decissions will be different.</p>

    <p>If we assume $R(s)=+2$, then we can expect the policy of the next figure. As the reward is possitive, the desired action doesn't want to finish so it tries to away from the terminating states. The ' $+$ ' symbol means that it can take any direction.</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&larr;</td>
        <td>&larr;</td>
        <td>&rarr;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&uarr;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&rarr;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&uarr;</td>
      </tr>
    </table> 

    <p>On the other hand, if we assume $R(s)=-2$, then we can expect the policy of the next image. As it can be seen the actions tries to go to the terminate states.</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&uarr;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&uarr;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&uarr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
      </tr>
    </table> 

    <h2>Sequences of Rewards and Assumptions</h2>

    <p>In the previous cases we assumed:</p>
    <ul>
      <li><b>Infinite horizons</b>. This means that we wanted to avoid going to the end as quickly as possible if I have rewards of a certain value, because the game didn't end until we got to an absorving state. This basically implied that we could live forever. Without this assumption: 1) the policy would have been different (With $R(s)=0.04$ maybe it could be better to take the risk of going near the -1 to arrive more quickly to the +1 cell). And, 2) the policy can change even though we were in the same state.</li>
      <li><b>Utility of sequences</b>: This means that I prefer one sequence of states over another sequence of states today, then I prefer that sequence over the same sequence of states tomorrow. This is called <b>stationary references</b>
    $$if\ U(s_0\ s_1\ s_2\ ...) > U(s_0\ s'_1\ s'_2\ ...)\\ then\ U(s_1\ s_2\ ...) > U(s'_1\ s'_2\ ...)$$
      </li>
    </ul>

    <p>The second assumption can be expressed:</p>

    <p>$$U(s_0\ s_1\ s_2\ ...) = \sum_{t=0}^{\infty}R(S_t)$$</p>
    
    <p>If we have two sequences of rewards, $R1(s)=\{+1\ +1\ +1\ +1\ +1\ \ +1...\}$ and $R2(s)=\{+1\ +1\ +2\ +1\ +2\ \ +1...\}$, neither one is better than the other. The reason of this is that both have an utility equal to infinity. <i>"if we're always going to be able to get positive rewards no matter what we do, then it doesn't matter what we do",</i> this is the existencial dilemma of being inmortal. $U(s_0\ s_1\ s_2\ ...) = \sum_{t=0}^{\infty}R(S_t) = \infty$</p>

    <p>If now we consider the next equation:</p>

    <p>$$U(s_0\ s_1\ s_2\ ...) = \sum_{t=0}^{\infty} \gamma^t R(S_t),\ 0 \le \gamma < 1$$</p>

    <p>then the utility it exponencially implodes, and now it becames a geometric series, with:</p>

    <p>$$U(s_0\ s_1\ s_2\ ...) \le \sum_{t=0}^{\infty} \gamma^t R_{max} = \frac{R_{max}}{1-\gamma} $$</p>

    <p>This is called <b>discounted rewards</b> or <b>discounted series</b> or <b>discounted sums</b>, and it allows us to go an infinite distance in finite time. So if we're discounted in this way, then that gives us a geometric series, that allows us to add an infinite number of numbers and gives us a finite number.</p>

    <p>If $\gamma$ is closed to 0, we are getting just one reward in the beginning and then everything falls off to nothing after all, but if $\gamma$ is really close to 1 we have a big reward, but not infinity.</p>

    <p>Despite $\gamma$, it is still consistent with our infinite horizons and our stationary over references. So with discounted rewards we are still consistent with the assumptions.  </p>


    <h2>Policies</h2>

    <p>The optimal policy is the one that maximizes our long-term expected reward:</p>

    <p>$$\pi^* = \underset{\pi}{argmax}\ \mathbb{E}\ \left[\sum_{t=0}^{\infty} \gamma^t R(S_t)\ |\ \pi \right]$$</p>

    <p>And the utility of a particular state is going to depend upon the policy that is following, and it is is going to be the expected set of states that it is going to see from that point on given that it has followed the policy:</p>

    <p>$$U^{\pi}(s)= \mathbb{E} \left[\sum_{t=0}^{\infty} \gamma^t R(S_t)\ |\ \pi,\ s_0=s\right]$$</p>

    <p>It is important to remember that the reward for entering a state is not the same than the utility for that state, $R(s) \neq U^{\pi}(s)$. The reward gives us immediate gratification or immediate feedback, but the utility gives us long term feedback.</p>

    <p> The optimal policy ais the one that for every state returns the action that maximizes my expected utility:</p>

    <p>$$\pi^*(s) = \underset{a}{argmax} \ \sum_{s'} T(s,a,s')U(s')$$</p>

    <p>Henceforth, The utility overstate is the utility of the state if I follow the optimal policy: $U(s)\equiv U^{\pi^*}$. This is called the true utility overstate.</p>

    <p>The <b>true utility overstate</b> is the reward that I get for being in that state plus a discount of the reward I'm goint to get from that point on. This relation is known as the <b>Bellman Equation</b>:</p>

    <p>$$U(s)=R(s)+\gamma \underset{a}{max} \sum_{s'} T(s,a,s')U(s')$$</p>

    <h2>Finding Policies</h2>

    <p>Considering Bellman equation $U(s)=R(s)+\gamma \underset{a}{max} \sum_{s'} T(s,a,s')U(s')$. If there are $n$ states, then there are $n$ bellman equations $n$ unknowns. Bellman equation is not linear due the $\max operation.$</p>

    <p>The <b>Value Iteration</b> algorithm is:</p>
    <ul>
      <li>Start with arbitrary utilities</li>
      <li>Update utilities based on neighbours</li>
      <li>Repeat until convergence</li>
    </ul>

    <p>The utilities are updated with:</p>

    <p>$$\hat{U}(s)_{t+1}=R(s)+\gamma \underset{a}{max} \sum_{s'} T(s,a,s')\hat{U}_t(s')$$</p>

    <p>The actual reward $R(s)$ is a true value, and these reward will be propagated to all of the states that I'm going to see, until it converge.</p>

    <p>Let's see an example, trying to calculate $U_1(x)$ and $U_1(x)$ for the cell with the letter X. Considering $\gamma=1/2$, $R(s)=-0.04$. And that the initial arbitrary utilities are zero for all cells, except by the two absorving states +1 and -1 cells, $U_0(s)=0 . Don't forget the transition probabilities: 0.8 of going in the desired direction, and 0.1 of going in each of the directions at 90-degrees. 
</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>X</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&nbsp;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&nbsp;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </table> 

    <p>Considering these, in $t=1$, between the 4 possible actions, it is obviously that go to right is the best action, so it is the most likely be the $max_a$. For this reason, with this action: $U_1(x)=-0.04+0.5(0.1*0+0.1*0+0.8*1)=0.36$</p>

    <p>In $t=2$, we have to calculate the same for the action of goint to right. But in this case it is also important to consider the value of $U_1$ for the cell bellow <var>x</var> (3,3). In this cell, the best option (so probably with most $\max_a$) is try to go to the left (that is not possible) here, $U_1(cell_{(2,3)})=-0.04+0.5(0.1*0+0.1*0+0.8*0)=-0.04$</p>.

    <p>Considering the above, in $t=2$, $U_2(x)=R(x) + \gamma (T(x,{right},cell_{(2,3)})*U_1(cell_{(2,3)}) +  T(x,{right},x)*U_1(x) + T(x,{right},cell_{4,3})*U_1(cell_{4,3}))$, $U_2(x)=-0.04+0.5(0.1*-0.04+0.1*0.36+0.8*1)=0.376$. Note that second term in the sum is $0.1*0.36$, the 0.36 value is because due it is not possible move upwards the next state $x'$ is the same than $x$.</p>

    <p>A policy is a mapping from state to action, it is not a mapping from states to utilities, so if we have <var>U</var>, we can figure out <var>&pi;</var>, but <var>U</var> is much more information than we need to figure out <var>&pi;</var>. If we have a <var>U</var> that is not the correct utility, but has the ordering of the actions correct, then we're actually doing pretty well. It doesn't matter we have the wrong utilities.</p>



    <p>Below the Algorithm <b>Policy Iteration</b> is presented:</p>

    <ul>
      <li>Start with an arbitrary $\pi_0$</li>
      <li><u>Evaluate</u>: given $\pi_t$ calculate its utility following that policy, $U_t=U^{\pi_t}$</li>
      <li><u>Improve</u>: $\pi_{t+1}=\underset{a}{argmax}\ \sum T(s,a,s')U_t(s')$</li>
    </ul>

    <p>The utility at time <var>t</var> is:</p>

    <p>$$U_t(s)=R(s)+\gamma \sum T(s,\pi_t(s),s')U_t(s')$$</p>

    <p>Due there is not the max in the equation above, it is a linear equation. So in a problem there are <var>n</var> linear equations in <var>n</var> unknowns.</p>

    <p>Because the algorithm is evaluated in policies space and not in value space, there are bigger jumps.</p>

    <h2>Summary</h2>
    <ul>
      <li>Markov Decision Processes (MDPs) are processes that consist of states, rewards, actions and transitions. With a discount parameter. In some sense, the states, the actions and the transitions represent the physical word and the discount represent the kind of task description.</li>
      <li>Policies</li>
      <li>Value functions (utilities). While the utilities factor is in long term aspect, and the rewards are at the moment.</li>
      <li>Discounting allow us to deal with infinite sequences, but threat them as if their value is finite.</li>
      <li>Stationary</li>
      <li>The Bellman equation can be implemented in value iteration and in policy iteration.</li>

    </ul>

    <div class="page-header">
      <h1 id="reinforcementLearning">Reinforcement Learning</h1>
    </div>
    
    <p>Thinking about a reinforcement learning "API", like application programmer interface. </p>

    <p>The idea of being able to take a model of an MDP, whick consists of a transition function, T, and a reward function R. If it goues trough some code and a policy comes out. This activity is called <b>Planning</b>.</p>

    <img src="../../images/tutorials/machinelearning/reinforcementlearning/reinforcementlearning-udacity/planner-API.png" height="96" width="469" alt="Planner-API">

    <p>If the API takes a bunch of samples of being in some state, taking some action, observing a reward and observing the state at the end of the transition, and the output is a policy, then this activity is a <b>Reinforcement Learning</b>.</p>

    <img src="../../images/tutorials/machinelearning/reinforcementlearning/reinforcementlearning-udacity/learner-API.png" height="96" width="469" alt="Learner-API">


    <h2>Rat Dinosaurs</h2>
    
    <p>If you put a rat in a box and give it choices of looking in place A and place B, with the doors closed and with a cheese in one of them. If we consistently turn on a red light whenever the cheese is in the A place, and a blue light whenever the cheese is in the B place, if you do...</p>

    <h2>API</h2>

    <h2>Three Approaches to RL</h2>

    <h2>A New Kind of Value Function</h2>

    <h2>Estimating Q from Transitions</h2>

    <h2>Q Learning Convergence</h2>

    <h2>Greedy Expoloration</h2>



  </div>
</section>


<section class="tutorial-block-intro" style="background-color:#F9F7EB;">
  <div class="container">
    <h2>Reference List</h2>
    <ol>
      <li><cite id="refUdacity"> Machine Learning: Reinforcement Learning. <small>Conversations on Analyzing Data </small>. <a href="https://www.udacity.com/course/ud820">Udacity UD820</a> </cite></li>
    </ol>
  </div>
</section>

<footer id="footerSocialMedia">
  <div class="container">
    <div class="row center-block">
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://www.linkedin.com/in/domingoesteban"><img class="footericon" src="../../images/icons/linkedin2-circle-logo-128.png" alt="Linkedin"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://github.com/domingoesteban"><img class="footericon" src="../../images/icons/github2-circle-logo-128.png" alt="Github"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://twitter.com/domingoesteban"><img class="footericon" src="../../images/icons/twitter2-circle-logo-128.png" alt="Twitter"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://google.com/+DomingoEsteban"><img class="footericon" src="../../images/icons/googleplus2-circle-logo-128.png" alt="Google Plus"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://www.youtube.com/user/domingoestebanc"><img class="footericon" src="../../images/icons/youtube2-circle-logo-128.png" alt="Youtube"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://www.facebook.com/domingoesteban"><img class="footericon" src="../../images/icons/facebook2-circle-logo-128.png" alt="Facebook"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://instagram.com/domingoesteban"><img class="footericon" src="../../images/icons/instagram2-circle-logo-128.png" alt="Instagram"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://vine.co/domingoesteban"><img class="footericon" src="../../images/icons/vine-circle-logo-128.png" alt="Vine"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://vimeo.com/domingoesteban"><img class="footericon" src="../../images/icons/vimeo-circle-logo-128.png" alt="Vimeo"/></a>
      </div>
    </div>
  </div>

  <div class="col-xs-12" id="footerCopyright">
    <h6 class="text-center">Copyright © 2014 Domingo Esteban</h6>
    <!-- <p class="text-center"><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a> </p> -->
  </div>

</footer>

<!-- JAVASCRIPT (At the bottom for faster page loading)-->

<!-- jQuery -->
<script src="https://code.jquery.com/jquery.js"></script> <!-- Online version 1 -->
<!--script src="http://ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script--> <!-- Online version 2 -->

<!-- ARREGLAR!!!! -->
<!--script>window.jQuery || document.write('<script src="includes/js/jquery.js"></script>')</script--> <!-- If no online access 1 -->
<!--script src="includes/js/jquery.js"> </script--> <!-- If no online access 2 -->

<!-- Bootstrap JS -->
<script src="../framework/js/bootstrap.js"> </script>

<!-- Custom JS -->
<script src="../includes/js/scripts.js"></script>

</body>

<!-- PAGINA PARA FORMATO DE REFERENCIAS: http://www.york.ac.uk/integrity/ieee.html -->

</html> 