<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>My "Reinforcement Learning" Notes - Domingo Esteban</title>
  <meta name="keywords" content="Domingo, Esteban, Cabala, Robotics, Peru, Spain, Madrid, Arequipa, Machine, Learning, Reinforcement">
  <meta name="description" content="Reinforcement Learning Notes - Domingo Esteban">
  <meta name="author" content="Domingo Esteban">

  <!-- Mobile viewport optimized -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"/>
  <!--meta name="viewport" content="width=device-width, initial-scale=1"-->

  <!-- Bootstrap CSS -->
  <!-- <link rel="stylesheet" href="../framework/css/bootstrap.css"> -->
  <link href='http://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css' rel='stylesheet'/>
  <link href='http://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css' rel='stylesheet'/>

  <!-- Custom CSS -->
  <link rel="stylesheet" href="../../includes/css/styles.css"/>
  <link rel="stylesheet" href="../../includes/css/tutorials-styles.css"/>

  <!-- Including Modernizr (Before any other Javascript) -->

  <!-- Favicon -->
  <link href="../../images/icons/robot-favicon.ico" rel="icon" type="image/x-icon" />

  <!-- MathJax  -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
  <script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>

</head>

<body>


<section class="tutorial-block-intro" style="background-color:#F9F7EB;">
  <div class="container">
    <h1 align="center">Reinforcement Learning</h1>

    <h2>The world</h2>

    <ul>
      <li><a href="#markovDecisionProcesses">Markov Decision Processes</a></li>
      <li><a href="#reinforcementLearning">Reinforcement Learning</a></li>
      <li><a href="#gameTheory">Game Theory</a></li>
    </ul>

    <div class="page-header">
      <h1 id="markovDecisionProcesses">Markov Decision Processes</h1>
    </div>

    <table class="table table-bordered text-center">
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&nbsp;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&nbsp;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
        <td>&nbsp;</td>
      </tr>
    </table> 

    <h2>Markov Decision Processes</h2>

    <p>The Markovian property in simple words means that only the present matter.</p>
    <p>The second property is that things are <i>stationary</i>. It means that rules don't change over time.</p>

    <ul>
      <li><b>States: $S$</b> . States are a set of tokens that represent every state that one could be in.</li>
      <li><b>Model: $T(s,a,s') \sim Pr(s'|s,a)$</b> . Also called the <i>Transition Model</i> or <i>Transition Function</i>, it is a function of 3 variables: a state, an action and another state. The function produces the probability that the agent will end up transitioning the state <var>s'</var> given the state <var>s</var> and the action <var>a</var>.</li>
      <li><b>Actions: $A(s)$, $A$</b> . The things that an agent can do in a particular state.</li>
      <li><b>Reward: $R(s)$, $R(s,a)$, $R(s,a,s')$</b> . It is a scalar value that you get for being in a state. There is also a notion of reward that you get for entering into a state and taking an action. And there is a reward that you get for being in a state, taking an action, and then ending in another state.</li>
      <li><b>Policy: $\pi (s) \rightarrow a $</b> . A policy is a function that takes in a state and returns an action. In other words, for any given state it tells you the action that you should take.</li>
    </ul>

    <p>The states, the model, the actions and the reward, along with the Markov properties, defines what is called the <b>Markov Decission Process</b> or <b>MDP</b> </p>

    <p>The solution to the Markov Decission Process is something called a Policy.</p>

    <p>Thre is an special policy, <b>$\pi^*$</b> or <b>Optimal Policy</b>, that is the policy that maximizes your long-term expected reward.</p>

    <p>If we want to learn a policy it is necessary a bunch of $\lt s,a\gt$ examples. With these, we can learn a function, the policy, that maps states to actions.</p>

    <p>But in the reinforcement world, int the MDP world, we see a sequence of states, actions and rewards $\lt s,a,r\gt$. In other words we have "if we are in the state <var>$s$</var>, and we take the action <var>$a$</var>, the reward that we see is <var>$r$</var> ", and from that we need to find the optimal action.</p>

    <p>A MDP, a policy dont tell you what sequence of actions (a plan) to take from a particular state, it tells you what action to take in a particular state. You will then end up in another state because of the transition model, the transition function. And then you are in that state you ask the policy what actions should I take now.</p>

    <h2>More About Rewards</h2>

    <p>There's a problem where you take some action that puts you in some place and then you take another action and that puts you in some other place, and maybe it puts you at a place where you get +1 or maybe it puts you at a place where you get -1. This reward is noy just an idea of getting a reward at every state, it's an idea of getting delayed reward. So you don't know how your immediate action is going to lead to things down the road.</p>

    <p>You get a sequence of $\lt s,a,r \gt $ triplets, and ultimately you have to figure out for the given state ( <var>$s$</var> ) you'are in, what was the action or actions ( <var>$a$</var> ) you took that helped to determinate the ultimate sequence of rewards that you saw. Perhaps the +1 or the -1. you got at the end. This is a difficult problem and as we are talking about a sequence of events over time its called <b>the credit assignment problem</b>.</p>

    <p>If we set the rewards to be equal to -0.04, $R(s) = -0.04 $ . So for the example, this is the reward that we are going to receive in every single state is -0.04, except for the two states +1 and -1 that ends the game (the <b>terminating</b> or <b>absorbing states</b>).
    </p>

    <p>If the world example now considers these sets of rewards:</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&uarr;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&uarr;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&uarr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
      </tr>
    </table> 

    <p>If we take a look at (3,1) cell, it has a left arrow because the policy tries to avoid the (3,2) cell that is to the right of the -1. Remember the world is stochastic, so there is a chance of moving to the right and following into the -1.</p>

    <p>So due the world has two assumptions: 1) delayed reward, and 2) minor changes matter, so if we have a slightly different reward, the decissions will be different.</p>

    <p>If we assume $R(s)=+2$, then we can expect the policy of the next figure. As the reward is possitive, the desired action doesn't want to finish so it tries to away from the terminating states. The ' $+$ ' symbol means that it can take any direction.</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&larr;</td>
        <td>&larr;</td>
        <td>&rarr;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&uarr;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&rarr;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&uarr;</td>
      </tr>
    </table> 

    <p>On the other hand, if we assume $R(s)=-2$, then we can expect the policy of the next image. As it can be seen the actions tries to go to the terminate states.</p>

    <table class="table table-bordered text-center">
      <tr>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td>&rarr;</td>
        <td bgcolor="#32cd32">+1</td>
      </tr>
      <tr>
      <tr>
        <td>&uarr;</td>
        <td bgcolor="#000000">&nbsp;</td>
        <td>&uarr;</td>
        <td bgcolor="#ff0000">-1</td>
      </tr>
      <tr>
        <td>&uarr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
        <td>&larr;</td>
      </tr>
    </table> 

    <h2>Sequences of Rewards and Assumptions</h2>

    <p>In the previous cases we assumed:</p>
    <ul>
      <li><b>Infinite horizons</b>. This means that we wanted to avoid going to the end as quickly as possible if I have rewards of a certain value, because the game didn't end until we got to an absorving state. This basically implied that we could live forever. Without this assumption: 1) the policy would have been different (With $R(s)=0.04$ maybe it could be better to take the risk of going near the -1 to arrive more quickly to the +1 cell). And, 2) the policy can change even though we were in the same state.</li>
      <li><b>Utility of sequences</b>: This means that I prefer one sequence of states over another sequence of states today, then I prefer that sequence over the same sequence of states tomorrow. This is called <b>stationary references</b>
    $$if\ U(s_0\ s_1\ s_2\ ...) > U(s_0\ s'_1\ s'_2\ ...)\\ then\ U(s_1\ s_2\ ...) > U(s'_1\ s'_2\ ...)$$
      </li>
    </ul>

    <p>The second assumption can be expressed:</p>

    <p>$$U(s_0\ s_1\ s_2\ ...) = \sum_{t=0}^{\infty}R(S_t)$$</p>
    
    <p>If we have two sequences of rewards, $R1(s)=\{+1\ +1\ +1\ +1\ +1\ \ +1...\}$ and $R2(s)=\{+1\ +1\ +2\ +1\ +2\ \ +1...\}$, neither one is better than the other. The reason of this is that both have an utility equal to infinity. <i>"if we're always going to be able to get positive rewards no matter what we do, then it doesn't matter what we do",</i> this is the existencial dilemma of being inmortal. $U(s_0\ s_1\ s_2\ ...) = \sum_{t=0}^{\infty}R(S_t) = \infty$</p>

    <p>If now we consider the next equation:</p>

    <p>$$U(s_0\ s_1\ s_2\ ...) = \sum_{t=0}^{\infty} \gamma^t R(S_t),\ 0 \le \gamma < 1$$</p>

    <p>then the utility it exponencially implodes, and now it becames a geometric series, with:</p>

    <p>$$U(s_0\ s_1\ s_2\ ...) \le \sum_{t=0}^{\infty} \gamma^t R_{max} = \frac{R_{max}}{1-\gamma} $$</p>

    <p>This is called <b>discounted rewards</b> or <b>discounted series</b> or <b>discounted sums</b>, and it allows us to go an infinite distance in finite time. So if we're discounted in this way, then that gives us a geometric series, that allows us to add an infinite number of numbers and gives us a finite number.</p>

    <p>If $\gamma$ is closed to 0, we are getting just one reward in the beginning and then everything falls off to nothing after all, but if $\gamma$ is really close to 1 we have a big reward, but not infinity.</p>

    <p>Despite $\gamma$, it is still consistent with our infinite horizons and our stationary over references. So with discounted rewards we are still consistent with the assumptions.  </p>


    <h2>Policies</h2>

    <p>The optimal policy is the one that maximizes our long-term expected reward:</p>

    <p>$$\pi^* = argmax_{\pi}\ \mathbb{E}\ \left[\sum_{t=0}^{\infty} \gamma^t R(S_t)\ |\ \pi \right]$$</p>

    <p>And the utility of a particular state is going to depend upon the policy that is following, and it is is going to be the expected set of states that it is going to see from that point on given that it has followed the policy:</p>

    <p>$$U^{\pi}(s)= \mathbb{E} \left[\sum_{t=0}^{\infty} \gamma^t R(S_t)\ |\ \pi,\ s_0=s\right]$$</p>

    <p>It is important to remember that the reward for entering a state is not the same than the utility for that state, $R(s) \neq U^{\pi}(s)$. The reward fives us immediate gratification or immediate feedback, but the utility gives us long term feedback.</p>

    <h2>Finding Policies</h2>

    <div class="page-header">
      <h1 id="reinforcementLearning">Reinforcement Learning</h1>
    </div>

  </div>
</section>


<section class="tutorial-block-intro" style="background-color:#F9F7EB;">
  <div class="container">
    <h2>Reference List</h2>
    <ol>
      <li><cite id="refUdacity"> Machine Learning: Reinforcement Learning. <small>Conversations on Analyzing Data </small>. <a href="https://www.udacity.com/course/ud820">Udacity UD820</a> </cite></li>
    </ol>
  </div>
</section>

<footer id="footerSocialMedia">
  <div class="container">
    <div class="row center-block">
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://www.linkedin.com/in/domingoesteban"><img class="footericon" src="../../images/icons/linkedin2-circle-logo-128.png" alt="Linkedin"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://github.com/domingoesteban"><img class="footericon" src="../../images/icons/github2-circle-logo-128.png" alt="Github"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://twitter.com/domingoesteban"><img class="footericon" src="../../images/icons/twitter2-circle-logo-128.png" alt="Twitter"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://google.com/+DomingoEsteban"><img class="footericon" src="../../images/icons/googleplus2-circle-logo-128.png" alt="Google Plus"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://www.youtube.com/user/domingoestebanc"><img class="footericon" src="../../images/icons/youtube2-circle-logo-128.png" alt="Youtube"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block">
        <a target="_blank" href="https://www.facebook.com/domingoesteban"><img class="footericon" src="../../images/icons/facebook2-circle-logo-128.png" alt="Facebook"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://instagram.com/domingoesteban"><img class="footericon" src="../../images/icons/instagram2-circle-logo-128.png" alt="Instagram"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://vine.co/domingoesteban"><img class="footericon" src="../../images/icons/vine-circle-logo-128.png" alt="Vine"/></a>
      </div>
      <div class="col-xs-4 col-sm-1 col-md-offset-1 text-center center-block" >
        <a target="_blank" href="https://vimeo.com/domingoesteban"><img class="footericon" src="../../images/icons/vimeo-circle-logo-128.png" alt="Vimeo"/></a>
      </div>
    </div>
  </div>

  <div class="col-xs-12" id="footerCopyright">
    <h6 class="text-center">Copyright Â© 2014 Domingo Esteban</h6>
    <!-- <p class="text-center"><small>Hosted on <a href="http://pages.github.com">GitHub Pages</a> </p> -->
  </div>

</footer>

<!-- JAVASCRIPT (At the bottom for faster page loading)-->

<!-- jQuery -->
<script src="https://code.jquery.com/jquery.js"></script> <!-- Online version 1 -->
<!--script src="http://ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script--> <!-- Online version 2 -->

<!-- ARREGLAR!!!! -->
<!--script>window.jQuery || document.write('<script src="includes/js/jquery.js"></script>')</script--> <!-- If no online access 1 -->
<!--script src="includes/js/jquery.js"> </script--> <!-- If no online access 2 -->

<!-- Bootstrap JS -->
<script src="../framework/js/bootstrap.js"> </script>

<!-- Custom JS -->
<script src="../includes/js/scripts.js"></script>

</body>

<!-- PAGINA PARA FORMATO DE REFERENCIAS: http://www.york.ac.uk/integrity/ieee.html -->

</html> 